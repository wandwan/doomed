{"cells":[{"cell_type":"code","execution_count":48,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-06-15T20:13:13.097105Z","iopub.status.busy":"2023-06-15T20:13:13.096385Z","iopub.status.idle":"2023-06-15T20:13:16.958144Z","shell.execute_reply":"2023-06-15T20:13:16.957252Z","shell.execute_reply.started":"2023-06-15T20:13:13.097081Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from itertools import combinations\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.model_selection import StratifiedKFold, KFold\n","from sklearn.linear_model import LogisticRegression, LinearRegression\n","from sklearn.metrics import roc_auc_score, accuracy_score, log_loss\n","import lightgbm as lgb\n","import xgboost as xgb\n","from catboost import CatBoost, CatBoostClassifier, Pool\n","\n","import gc\n","import random\n","import optuna\n","optuna.logging.set_verbosity(optuna.logging.WARNING)\n","from copy import deepcopy\n","from functools import partial\n","\n","from scipy.optimize import minimize\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Read in data files"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-15T20:13:16.960315Z","iopub.status.busy":"2023-06-15T20:13:16.959814Z","iopub.status.idle":"2023-06-15T20:13:17.020167Z","shell.execute_reply":"2023-06-15T20:13:17.018846Z","shell.execute_reply.started":"2023-06-15T20:13:16.960285Z"},"trusted":true},"outputs":[],"source":["filepath = '/kaggle/input/icr-identify-age-related-conditions'\n","df_train = pd.read_csv(os.path.join(filepath, 'train.csv'), index_col='Id')\n","df_test = pd.read_csv(os.path.join(filepath, 'test.csv'), index_col=\"Id\")\n","greeks = pd.read_csv(os.path.join(filepath, 'greeks.csv'), index_col=\"Id\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Prepare training and testing datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-15T20:13:17.022058Z","iopub.status.busy":"2023-06-15T20:13:17.021581Z","iopub.status.idle":"2023-06-15T20:13:17.102369Z","shell.execute_reply":"2023-06-15T20:13:17.101079Z","shell.execute_reply.started":"2023-06-15T20:13:17.022035Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train shape :(617, 56) , y_train shape :(617,)\n","X_test shape :(5, 56)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>AB</th>\n","      <th>AF</th>\n","      <th>AH</th>\n","      <th>AM</th>\n","      <th>AR</th>\n","      <th>AX</th>\n","      <th>AY</th>\n","      <th>AZ</th>\n","      <th>BC</th>\n","      <th>BD</th>\n","      <th>...</th>\n","      <th>FI</th>\n","      <th>FL</th>\n","      <th>FR</th>\n","      <th>FS</th>\n","      <th>GB</th>\n","      <th>GE</th>\n","      <th>GF</th>\n","      <th>GH</th>\n","      <th>GI</th>\n","      <th>GL</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.209377</td>\n","      <td>3109.03329</td>\n","      <td>85.200147</td>\n","      <td>22.394407</td>\n","      <td>8.138688</td>\n","      <td>0.699861</td>\n","      <td>0.025578</td>\n","      <td>9.812214</td>\n","      <td>5.555634</td>\n","      <td>4126.58731</td>\n","      <td>...</td>\n","      <td>3.583450</td>\n","      <td>7.298162</td>\n","      <td>1.73855</td>\n","      <td>0.094822</td>\n","      <td>11.339138</td>\n","      <td>72.611063</td>\n","      <td>2003.810319</td>\n","      <td>22.136229</td>\n","      <td>69.834944</td>\n","      <td>0.120343</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.145282</td>\n","      <td>978.76416</td>\n","      <td>85.200147</td>\n","      <td>36.968889</td>\n","      <td>8.138688</td>\n","      <td>3.632190</td>\n","      <td>0.025578</td>\n","      <td>13.517790</td>\n","      <td>1.229900</td>\n","      <td>5496.92824</td>\n","      <td>...</td>\n","      <td>10.358927</td>\n","      <td>0.173229</td>\n","      <td>0.49706</td>\n","      <td>0.568932</td>\n","      <td>9.292698</td>\n","      <td>72.611063</td>\n","      <td>27981.562750</td>\n","      <td>29.135430</td>\n","      <td>32.131996</td>\n","      <td>21.978000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.470030</td>\n","      <td>2635.10654</td>\n","      <td>85.200147</td>\n","      <td>32.360553</td>\n","      <td>8.138688</td>\n","      <td>6.732840</td>\n","      <td>0.025578</td>\n","      <td>12.824570</td>\n","      <td>1.229900</td>\n","      <td>5135.78024</td>\n","      <td>...</td>\n","      <td>11.626917</td>\n","      <td>7.709560</td>\n","      <td>0.97556</td>\n","      <td>1.198821</td>\n","      <td>37.077772</td>\n","      <td>88.609437</td>\n","      <td>13676.957810</td>\n","      <td>28.022851</td>\n","      <td>35.192676</td>\n","      <td>0.196941</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.252107</td>\n","      <td>3819.65177</td>\n","      <td>120.201618</td>\n","      <td>77.112203</td>\n","      <td>8.138688</td>\n","      <td>3.685344</td>\n","      <td>0.025578</td>\n","      <td>11.053708</td>\n","      <td>1.229900</td>\n","      <td>4169.67738</td>\n","      <td>...</td>\n","      <td>14.852022</td>\n","      <td>6.122162</td>\n","      <td>0.49706</td>\n","      <td>0.284466</td>\n","      <td>18.529584</td>\n","      <td>82.416803</td>\n","      <td>2094.262452</td>\n","      <td>39.948656</td>\n","      <td>90.493248</td>\n","      <td>0.155829</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.380297</td>\n","      <td>3733.04844</td>\n","      <td>85.200147</td>\n","      <td>14.103738</td>\n","      <td>8.138688</td>\n","      <td>3.942255</td>\n","      <td>0.054810</td>\n","      <td>3.396778</td>\n","      <td>102.151980</td>\n","      <td>5728.73412</td>\n","      <td>...</td>\n","      <td>13.666727</td>\n","      <td>8.153058</td>\n","      <td>48.50134</td>\n","      <td>0.121914</td>\n","      <td>16.408728</td>\n","      <td>146.109943</td>\n","      <td>8524.370502</td>\n","      <td>45.381316</td>\n","      <td>36.262628</td>\n","      <td>0.096614</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 56 columns</p>\n","</div>"],"text/plain":["         AB          AF          AH         AM        AR        AX        AY  \\\n","0  0.209377  3109.03329   85.200147  22.394407  8.138688  0.699861  0.025578   \n","1  0.145282   978.76416   85.200147  36.968889  8.138688  3.632190  0.025578   \n","2  0.470030  2635.10654   85.200147  32.360553  8.138688  6.732840  0.025578   \n","3  0.252107  3819.65177  120.201618  77.112203  8.138688  3.685344  0.025578   \n","4  0.380297  3733.04844   85.200147  14.103738  8.138688  3.942255  0.054810   \n","\n","          AZ          BC         BD   ...         FI        FL        FR  \\\n","0   9.812214    5.555634  4126.58731  ...   3.583450  7.298162   1.73855   \n","1  13.517790    1.229900  5496.92824  ...  10.358927  0.173229   0.49706   \n","2  12.824570    1.229900  5135.78024  ...  11.626917  7.709560   0.97556   \n","3  11.053708    1.229900  4169.67738  ...  14.852022  6.122162   0.49706   \n","4   3.396778  102.151980  5728.73412  ...  13.666727  8.153058  48.50134   \n","\n","         FS         GB          GE            GF         GH         GI  \\\n","0  0.094822  11.339138   72.611063   2003.810319  22.136229  69.834944   \n","1  0.568932   9.292698   72.611063  27981.562750  29.135430  32.131996   \n","2  1.198821  37.077772   88.609437  13676.957810  28.022851  35.192676   \n","3  0.284466  18.529584   82.416803   2094.262452  39.948656  90.493248   \n","4  0.121914  16.408728  146.109943   8524.370502  45.381316  36.262628   \n","\n","          GL  \n","0   0.120343  \n","1  21.978000  \n","2   0.196941  \n","3   0.155829  \n","4   0.096614  \n","\n","[5 rows x 56 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df_train['EJ'] = df_train['EJ'].replace({'A': 0, 'B': 1})\n","df_test['EJ']  = df_test['EJ'].replace({'A': 0, 'B': 1})\n","#data = pd.concat([df_train, greeks], axis=1)\n","df_train.fillna(df_train.mean(), inplace=True)\n","target_col = 'Class'\n","\n","X_train = df_train.drop([target_col],axis=1).reset_index(drop=True)\n","Y_train = df_train[target_col].reset_index(drop=True)\n","X_test = df_test.reset_index(drop=True)\n","\n","#drop_cols = ['BC', 'CL']\n","#X_train.drop(drop_cols, axis=1, inplace=True)\n","#X_test.drop(drop_cols, axis=1, inplace=True)\n","\n","# Only 'EJ' is a categorical data so far\n","#numeric_columns = [_ for _ in X_train.columns if _ not in ['EJ']]\n","#scaler = StandardScaler() # MinMaxScaler or StandardScaler\n","#X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])\n","#X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])\n","\n","print(f\"X_train shape :{X_train.shape} , y_train shape :{Y_train.shape}\")\n","print(f\"X_test shape :{X_test.shape}\")\n","\n","# Delete the train and test dataframes to free up memory\n","# del df_train, df_test\n","\n","X_train.head(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Data Split and Hyperparameter config"]},{"cell_type":"code","execution_count":99,"metadata":{"execution":{"iopub.execute_input":"2023-06-13T20:55:32.973736Z","iopub.status.busy":"2023-06-13T20:55:32.973312Z","iopub.status.idle":"2023-06-13T20:55:32.999460Z","shell.execute_reply":"2023-06-13T20:55:32.998100Z","shell.execute_reply.started":"2023-06-13T20:55:32.973706Z"},"trusted":true},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax. Perhaps you forgot a comma? (2107267446.py, line 114)","output_type":"error","traceback":["\u001b[1;36m  Cell \u001b[1;32mIn[99], line 114\u001b[1;36m\u001b[0m\n\u001b[1;33m    'random_state': self.random_state\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"]}],"source":["class Splitter:\n","    def __init__(self, n_splits = 5):\n","        self.n_splits = n_splits\n","        \n","    def split_data(self, X, y, random_state_list):\n","        for random_state in random_state_list:\n","            # shuffle = True ???\n","            kfold = StratifiedKFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n","            for tr_index, val_index in kfold.split(X, y):\n","                yield tr_index, val_index\n","\n","class Classifier:\n","    def __init__(self, n_estimators = 100, device = 'cpu', random_state = 0):\n","        self.n_estimators = n_estimators\n","        self.device = device\n","        self.random_state = random_state\n","        self.models = self._define_models()\n","        self.models_name = list(self.models.keys())\n","        self.len_models = len(self.models)\n","        \n","    \n","    def _define_models(self):\n","        logistic_param = {\n","            'random_state': self.random_state,\n","            'penalty': 'l2',\n","            #'l1_ratio': 0\n","        }\n","        \n","        xgb1_params = {\n","            'n_estimators': self.n_estimators,\n","            'learning_rate': 0.413327571405248,\n","            'booster': 'gbtree',\n","            'lambda': 0.0000263894617720096,\n","            'alpha': 0.000463768723479341,\n","            'subsample': 0.237467672874133,\n","            'colsample_bytree': 0.618829300507829,\n","            'max_depth': 5,\n","            'min_child_weight': 9,\n","            'eta': 2.09477807126539E-06,\n","            'gamma': 0.000847289463422307,\n","            'grow_policy': 'depthwise',\n","            'n_jobs': -1,\n","            'objective': 'binary:logistic',\n","            'eval_metric': 'logloss',\n","            'verbosity': 0,\n","            'random_state': self.random_state,\n","        }\n","        if self.device == 'gpu':\n","            xgb_params['tree_method'] = 'gpu_hist'\n","            xgb_params['predictor'] = 'gpu_predictor'\n","        \n","        lgb1_params = {\n","            'n_estimators': self.n_estimators,\n","            'objective': 'binary',\n","            'boosting_type': 'gbdt',\n","            'learning_rate': 0.005,\n","            'num_leaves': 5,\n","            'colsample_bytree': 0.50,\n","            'subsample': 0.80,\n","            'reg_alpha': 2, \n","            'reg_lambda': 4,\n","            'n_jobs': -1,\n","            'is_unbalance':True,\n","            'device': self.device,\n","            'random_state': self.random_state\n","        }\n","        lgb2_params = {\n","            'n_estimators': self.n_estimators,\n","            'learning_rate': 0.190197487721534,\n","            'reg_alpha': 0.00749112221417973,\n","            'reg_lambda': 0.000548118227209224,\n","            'num_leaves': 17,\n","            'colsample_bytree': 0.547257860506146,\n","            'subsample': 0.592628085686409,\n","            'subsample_freq': 2,\n","            'min_child_samples': 64,\n","            'objective': 'binary',\n","            #'metric': 'binary_error',\n","            'boosting_type': 'gbdt',\n","            'is_unbalance':True,\n","            'device': self.device,\n","            'random_state': self.random_state\n","        } \n","        lgb3_params = {\n","            'n_estimators': self.n_estimators,\n","            'learning_rate': 0.181326407627473,\n","            'reg_alpha': 0.000030864084239014,\n","            'reg_lambda': 0.0000395714763869486,\n","            'num_leaves': 122,\n","            'colsample_bytree': 0.75076596295323,\n","            'subsample': 0.6303245788342,\n","            'subsample_freq': 3,\n","            'min_child_samples': 72,\n","            'objective': 'binary',\n","            #'metric': 'binary_error',\n","            'boosting_type': 'gbdt',\n","            'is_unbalance':True,\n","            'device': self.device,\n","            'random_state': self.random_state\n","        }\n","        lgb5_params = {\n","            'n_estimators': self.n_estimators,\n","            'objective': 'binary',\n","            'boosting_type': 'gbdt',\n","            'n_jobs': -1,\n","            'is_unbalance':True,\n","            'device': self.device,\n","            'random_state': self.random_state\n","            'colsample_bytree': 0.8,\n","            'learning_rate': 0.1,\n","            'max_depth': 5, \n","            'num_leaves': 10, \n","            'reg_alpha': 0.0, \n","            'reg_lambda': 0.0, \n","            'subsample': 0.8\n","        }\n","        cat1_params = {\n","            'iterations': self.n_estimators,\n","            'colsample_bylevel': 0.0513276895988184,\n","            'depth': 2,\n","            'learning_rate': 0.0256579773375401,\n","            'l2_leaf_reg': 8.22319805476255,\n","            'random_strength': 0.11327724457066,\n","            'od_type': \"Iter\", \n","            'od_wait': 72,\n","            'bootstrap_type': \"Bayesian\",\n","            'grow_policy': 'SymmetricTree',\n","            'bagging_temperature': 9.58737431845122,\n","            #'eval_metric': 'Logloss',\n","            #'loss_function': 'Logloss',\n","            'auto_class_weights': 'Balanced',\n","            'task_type': self.device.upper(),\n","            'random_state': self.random_state\n","        }\n","        \n","        study = optuna.create_study(direction='minimize')\n","        study.optimize(self.lgb_optuna, n_trials=20)\n","        lgb4_params = study.best_params\n","        lgb4_params['objective'] = 'binary'\n","        #lgb4_params['learning_rate'] = 0.1\n","        lgb4_params['is_unbalance'] = True\n","        \n","        models = {\n","            #'logistic': LogisticRegression(**logistic_param),\n","            #'xgb1': xgb.XGBClassifier(**xgb1_params),\n","            'lgb1': lgb.LGBMClassifier(**lgb1_params),\n","            #'lgb2': lgb.LGBMClassifier(**lgb2_params),\n","            #'lgb3': lgb.LGBMClassifier(**lgb3_params),\n","            'lgb4': lgb.LGBMClassifier(**lgb4_params),\n","            #'cat1': CatBoostClassifier(**cat1_params),\n","        }\n","        \n","        return models\n","    \n","    def lgb_optuna(self, trial):\n","        params = {\n","            'objective': 'binary',\n","            #'metric': 'gbdt',\n","            'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.5),\n","            'num_leaves': trial.suggest_int('num_leaves', 3, 100),\n","            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n","            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n","            'max_depth':trial.suggest_int('max_depth', 1, 20),\n","            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n","            'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 10.0),\n","            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0),\n","            'is_unbalance':True\n","        }\n","\n","        score_list = []\n","\n","        for fold, (train_idx, val_idx) in enumerate(StratifiedKFold(n_splits=5, random_state=42, shuffle=True).split(X_train, Y_train)):\n","            x_tr, x_va = X_train.loc[train_idx], X_train.loc[val_idx]\n","            y_tr, y_va = Y_train.loc[train_idx], Y_train.loc[val_idx]\n","            train_w0, train_w1 = calc_log_loss_weight(y_tr)\n","            valid_w0, valid_w1 = calc_log_loss_weight(y_va)\n","\n","            model = lgb.LGBMClassifier(**params)\n","            model.fit(x_tr, y_tr, sample_weight=y_tr.map({0: train_w0, 1: train_w1}), \n","                      eval_set=[(x_va,y_va)], eval_sample_weight=[y_va.map({0: valid_w0, 1: valid_w1})],\n","                      verbose=0, early_stopping_rounds=500)\n","\n","            y_va_pred = model.predict_proba(x_va)[:, 1].reshape(-1)\n","            score = balanced_log_loss(y_va, y_va_pred)\n","            score_list.append(score)\n","\n","        return sum(score_list) / len(score_list)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Metrics"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2023-06-13T20:55:33.002161Z","iopub.status.busy":"2023-06-13T20:55:33.001831Z","iopub.status.idle":"2023-06-13T20:55:33.015531Z","shell.execute_reply":"2023-06-13T20:55:33.014207Z","shell.execute_reply.started":"2023-06-13T20:55:33.002133Z"},"trusted":true},"outputs":[],"source":["def calc_log_loss_weight(y_true):\n","    nc = np.bincount(y_true)\n","    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n","    # w0, w1 = 1/nc[0], 1/nc[1]\n","    return w0, w1\n","\n","def balanced_log_loss(y_true, y_pred):\n","    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n","    nc = np.bincount(y_true)\n","    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n","    balanced_log_loss_score = (-w0/nc[0]*(np.sum(np.where(y_true==0,1,0) * np.log(1-y_pred))) - w1/nc[1]*(np.sum(np.where(y_true!=0,1,0) * np.log(y_pred)))) / (w0+w1)\n","    return balanced_log_loss_score\n","\n","def jeremy_logloss(y_true, y_pred):\n","    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n","    n0 = (y_true==0).sum()\n","    n1 = (y_true==1).sum()\n","    if not n1*np.log(y_pred[y_true==1]).sum() == 0 and not n0*np.log(1-y_pred[y_true==0]).sum() == 0:\n","        logloss = ( -1/n0*np.log(1-y_pred[y_true==0]).sum()  - 1/n1*np.log(y_pred[y_true==1]).sum() ) / 2\n","    return logloss\n","\n","def adj_pred_logloss(k, pred):\n","    t = 1* ( pred > np.random.uniform(0,1,size=len(pred)))\n","    odds = pred / ( 1 - pred )\n","    adj_odds = k[0] * odds\n","    adj_pred = adj_odds / ( 1 + adj_odds )\n","    return jeremy_logloss(t, adj_pred)\n","\n","def post(pred):\n","    x1 = np.random.random((5,))\n","    x2 = np.random.random((5,))*2\n","    x3 = np.random.random((5,))*5\n","    x4 = np.random.random((5,))*10\n","    x5 = np.random.random((5,))*.1\n","    x0 = np.hstack((x1,x2,x3,x4,x5))\n","    foo = minimize(adj_pred_logloss, x0=x0, args=(pred))\n","    return foo.x"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2023-06-13T20:55:33.019718Z","iopub.status.busy":"2023-06-13T20:55:33.019356Z","iopub.status.idle":"2023-06-13T20:57:41.046408Z","shell.execute_reply":"2023-06-13T20:57:41.045244Z","shell.execute_reply.started":"2023-06-13T20:55:33.019688Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["************** Start Training Fold-0 ******************\n","lgb1 [REPEAT-0 FOLD-0 SEED-1824] BalancedLogLoss Validation score: 0.23898, Training score: 0.02691\n","lgb4 [REPEAT-0 FOLD-0 SEED-1824] BalancedLogLoss Validation score: 0.25048, Training score: 0.03386\n","\n","\n","************** Start Training Fold-1 ******************\n","lgb1 [REPEAT-0 FOLD-1 SEED-1824] BalancedLogLoss Validation score: 0.22139, Training score: 0.02336\n","lgb4 [REPEAT-0 FOLD-1 SEED-1824] BalancedLogLoss Validation score: 0.23716, Training score: 0.11124\n","\n","\n","************** Start Training Fold-2 ******************\n","lgb1 [REPEAT-0 FOLD-2 SEED-1824] BalancedLogLoss Validation score: 0.24717, Training score: 0.02511\n","lgb4 [REPEAT-0 FOLD-2 SEED-1824] BalancedLogLoss Validation score: 0.23241, Training score: 0.04833\n","\n","\n","************** Start Training Fold-3 ******************\n","lgb1 [REPEAT-0 FOLD-3 SEED-1824] BalancedLogLoss Validation score: 0.34282, Training score: 0.02051\n","lgb4 [REPEAT-0 FOLD-3 SEED-1824] BalancedLogLoss Validation score: 0.33350, Training score: 0.03617\n","\n","\n","************** Start Training Fold-4 ******************\n","lgb1 [REPEAT-0 FOLD-4 SEED-1824] BalancedLogLoss Validation score: 0.33804, Training score: 0.03779\n","lgb4 [REPEAT-0 FOLD-4 SEED-1824] BalancedLogLoss Validation score: 0.26700, Training score: 0.06154\n","\n","\n","lgb1 SEED-1824] BalancedLogLoss Total CV score: 0.27629\n","lgb4 SEED-1824] BalancedLogLoss Total CV score: 0.26300\n","\n","\n","************** STACKING ******************\n","oof stack factor: 0.32559557487102875\n","[Stacking SEED-1824] BalancedLogLoss Total CV score with adjustment: 0.26533\n","Stacking SEED-1824] BalancedLogLoss Total CV score: 0.27629\n","CPU times: total: 4min 44s\n","Wall time: 28.3 s\n"]}],"source":["%%time\n","\n","n_splits = 5\n","n_reapts = 1\n","random_state = 42\n","n_estimators = 99999\n","early_stopping_rounds = 1000\n","verbose = False\n","device = 'cpu'\n","\n","# Fix seed\n","random.seed(random_state)\n","random_state_list = random.sample(range(9999), n_reapts)\n","\n","# Initialize an array for storing test predictions\n","classifier = Classifier(n_estimators, device, random_state)\n","oof_stack = pd.DataFrame(np.zeros(X_train.shape[0]), columns=['oof_stack'])\n","test_stack = pd.DataFrame(np.zeros(X_test.shape[0]), columns=['test_stack'])\n","oof_pred = pd.DataFrame(np.zeros((X_train.shape[0], classifier.len_models)), columns=classifier.models_name)\n","test_pred = pd.DataFrame(np.zeros((X_test.shape[0], classifier.len_models)), columns=classifier.models_name)\n","trained_models = {'xgb':[], 'cat':[]}\n","score_dict = dict(zip(classifier.models_name, [[] for _ in range(classifier.len_models)]))\n","\n","splitter = Splitter(n_splits=n_splits)\n","for i, (tr_idx, val_idx) in enumerate(splitter.split_data(X_train, Y_train, random_state_list=random_state_list)):\n","    x_train, x_val = X_train.loc[tr_idx], X_train.loc[val_idx]\n","    y_train, y_val = Y_train.loc[tr_idx], Y_train.loc[val_idx]\n","    n = i % n_splits\n","    m = i // n_splits\n","            \n","    # Get a set of classifier models\n","    classifier = Classifier(n_estimators, device, random_state_list[m])\n","    models = classifier.models\n","    \n","    # Initialize lists to store oof and test predictions for each base model\n","    \n","    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n","    print(f'************** Start Training Fold-{n} ******************')\n","    for name, model in models.items():\n","        if ('xgb' in name) or ('lgb' in name) or ('cat' in name):\n","            train_w0, train_w1 = calc_log_loss_weight(y_train)\n","            valid_w0, valid_w1 = calc_log_loss_weight(y_val)\n","            if 'xgb' in name:\n","                model.fit(\n","                    x_train, y_train, sample_weight=y_train.map({0: train_w0, 1: train_w1}), \n","                    eval_set=[(x_val, y_val)], sample_weight_eval_set=[y_val.map({0: valid_w0, 1: valid_w1})],\n","                    early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n","            elif 'lgb' in name:\n","                model.fit(\n","                    x_train, y_train, sample_weight=y_train.map({0: train_w0, 1: train_w1}), \n","                    eval_set=[(x_val, y_val)], eval_sample_weight=[y_val.map({0: valid_w0, 1: valid_w1})],\n","                    early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n","            elif 'cat' in name:\n","                model.fit(\n","                    Pool(x_train, y_train, weight=y_train.map({0: train_w0, 1: train_w1})), \n","                    eval_set=Pool(x_val, y_val, weight=y_val.map({0: valid_w0, 1: valid_w1})), \n","                    early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n","        else:\n","            model.fit(x_train, y_train)\n","            \n","        if name in trained_models.keys():\n","            trained_models[f'{name}'].append(deepcopy(model))\n","        \n","        y_train_pred = model.predict_proba(x_train)[:, 1].reshape(-1)\n","\n","\n","        y_val_pred = model.predict_proba(x_val)[:, 1].reshape(-1)\n","        y_test_pred = model.predict_proba(X_test)[:, 1].reshape(-1)\n","        \n","        train_score = balanced_log_loss(y_train, y_train_pred)\n","        val_score = balanced_log_loss(y_val, y_val_pred)\n","        #score_dict[name].append(score)\n","        \n","        print(f'{name} [REPEAT-{m} FOLD-{n} SEED-{random_state_list[m]}] BalancedLogLoss Validation score: {val_score:.5f}, Training score: {train_score:.5f}')\n","        \n","        #oof_preds.append(y_val_pred)\n","        oof_pred[name].loc[x_val.index] = y_val_pred\n","        #test_preds.append(test_pred)\n","        test_pred[name] += y_test_pred / n_splits\n","    \n","    print('\\n')\n","\n","for name in classifier.models_name:\n","    cv_score = balanced_log_loss(Y_train, oof_pred[name])\n","    print(f'{name} SEED-{random_state_list[m]}] BalancedLogLoss Total CV score: {cv_score:.5f}')\n","#oof_each_predss = np.mean(np.array(oof_each_predss), axis=0)\n","#test_each_predss = np.mean(np.array(test_each_predss), axis=0)\n","#oof_each_predss = np.concatenate([oof_each_predss, np.mean(oof_predss, axis=1).reshape(-1, 1)], axis=1)\n","#test_each_predss = np.concatenate([test_each_predss, test_predss.reshape(-1, 1)], axis=1)\n","\n","# Comducted a simple linear regression to stack all the models\n","print('\\n')\n","print(f'************** STACKING ******************')\n","#lr = LinearRegression(positive=True, fit_intercept=False).fit(oof_pred, Y_train)\n","oof_stack['oof_stack'] = oof_pred['lgb1']#lr.predict(oof_pred) / sum(lr.coef_)\n","#postprocess oof stack\n","factor = post(oof_stack['oof_stack'])[0]\n","print(f\"oof stack factor: {factor}\")\n","\n","odds = (oof_stack['oof_stack']) / ( 1 - oof_stack['oof_stack']) \n","adj_odds = factor * oof_stack['oof_stack']\n","adj_oof_stack = adj_odds / ( 1 + adj_odds )\n","adj_stack_score = balanced_log_loss(Y_train, adj_oof_stack)\n","print(f'[Stacking SEED-{random_state_list[m]}] BalancedLogLoss Total CV score with adjustment: {stack_score:.5f}')\n","\n","\n","test_stack['test_stack'] = test_pred['lgb4']#lr.predict(test_pred) / sum(lr.coef_) \n","stack_score = balanced_log_loss(Y_train, oof_stack['oof_stack'])\n","print(f'Stacking SEED-{random_state_list[m]}] BalancedLogLoss Total CV score: {stack_score:.5f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# hyperparameter tuning"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 4860 candidates, totalling 24300 fits\n","Best Hyperparameters:  {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'num_leaves': 10, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 0.8}\n","Best Score:  nan\n"]}],"source":["from sklearn.model_selection import GridSearchCV\n","\n","# Define the parameter grid\n","param_grid = {\n","    'num_leaves': [10, 20, 30],\n","    'max_depth': [5, 10, 15],\n","    'learning_rate': [0.1, 0.01, 0.001],\n","    'subsample': [0.8, 0.9, 1.0],\n","    'colsample_bytree': [0.8, 0.9, 1.0],\n","    'reg_alpha': [0.0, 0.1, 0.5, 7],\n","    'reg_lambda': [0.0, 0.1, 0.5,4, 6],\n","}\n","\n","\n","# Perform grid search with cross-validation\n","grid_search = GridSearchCV(estimator=models['lgb4'], param_grid=param_grid, cv=5, scoring=balanced_log_loss, verbose=2, n_jobs=-1)\n","grid_search.fit(x_train, y_train)\n","\n","# Print the best hyperparameters and the corresponding score\n","print(\"Best Hyperparameters: \", grid_search.best_params_)\n","print(\"Best Score: \", grid_search.best_score_)"]},{"cell_type":"code","execution_count":91,"metadata":{"execution":{"iopub.execute_input":"2023-06-13T20:57:41.048415Z","iopub.status.busy":"2023-06-13T20:57:41.048104Z","iopub.status.idle":"2023-06-13T20:57:41.066832Z","shell.execute_reply":"2023-06-13T20:57:41.065639Z","shell.execute_reply.started":"2023-06-13T20:57:41.048389Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>class_0</th>\n","      <th>class_1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00eed32682bb</td>\n","      <td>0.722936</td>\n","      <td>0.277064</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>010ebe33f668</td>\n","      <td>0.722936</td>\n","      <td>0.277064</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>02fa521e1838</td>\n","      <td>0.722936</td>\n","      <td>0.277064</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>040e15f562a2</td>\n","      <td>0.722936</td>\n","      <td>0.277064</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>046e85c7cc7f</td>\n","      <td>0.722936</td>\n","      <td>0.277064</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             Id   class_0   class_1\n","0  00eed32682bb  0.722936  0.277064\n","1  010ebe33f668  0.722936  0.277064\n","2  02fa521e1838  0.722936  0.277064\n","3  040e15f562a2  0.722936  0.277064\n","4  046e85c7cc7f  0.722936  0.277064"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["sub = pd.read_csv(os.path.join(filepath, 'sample_submission.csv'))\n","\n","sub['class_1'] = test_stack['test_stack']\n","sub['class_0'] = 1 - test_stack['test_stack']\n","sub.to_csv('submission.csv', index=False)\n","sub\n","\n","#0.23697\n","#0.23414\n","#0.24283"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
