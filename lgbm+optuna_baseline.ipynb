{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom itertools import combinations\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoost, CatBoostClassifier, Pool\n\nimport gc\nimport random\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nfrom copy import deepcopy\nfrom functools import partial\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-15T20:13:13.096385Z","iopub.execute_input":"2023-06-15T20:13:13.097105Z","iopub.status.idle":"2023-06-15T20:13:16.958144Z","shell.execute_reply.started":"2023-06-15T20:13:13.097081Z","shell.execute_reply":"2023-06-15T20:13:16.957252Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv\n/kaggle/input/icr-identify-age-related-conditions/greeks.csv\n/kaggle/input/icr-identify-age-related-conditions/train.csv\n/kaggle/input/icr-identify-age-related-conditions/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Read in data files","metadata":{}},{"cell_type":"code","source":"filepath = '/kaggle/input/icr-identify-age-related-conditions'\ndf_train = pd.read_csv(os.path.join(filepath, 'train.csv'), index_col='Id')\ndf_test = pd.read_csv(os.path.join(filepath, 'test.csv'), index_col=\"Id\")\ngreeks = pd.read_csv(os.path.join(filepath, 'greeks.csv'), index_col=\"Id\")","metadata":{"execution":{"iopub.status.busy":"2023-06-15T20:13:16.959814Z","iopub.execute_input":"2023-06-15T20:13:16.960315Z","iopub.status.idle":"2023-06-15T20:13:17.020167Z","shell.execute_reply.started":"2023-06-15T20:13:16.960285Z","shell.execute_reply":"2023-06-15T20:13:17.018846Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Prepare training and testing datasets","metadata":{}},{"cell_type":"code","source":"df_train['EJ'] = df_train['EJ'].replace({'A': 0, 'B': 1})\ndf_test['EJ']  = df_test['EJ'].replace({'A': 0, 'B': 1})\n#data = pd.concat([df_train, greeks], axis=1)\ndf_train.fillna(df_train.mean(), inplace=True)\ntarget_col = 'Class'\n\nX_train = df_train.drop([target_col],axis=1).reset_index(drop=True)\nY_train = df_train[target_col].reset_index(drop=True)\nX_test = df_test.reset_index(drop=True)\n\n#drop_cols = ['BC', 'CL']\n#X_train.drop(drop_cols, axis=1, inplace=True)\n#X_test.drop(drop_cols, axis=1, inplace=True)\n\n# Only 'EJ' is a categorical data so far\n#numeric_columns = [_ for _ in X_train.columns if _ not in ['EJ']]\n#scaler = StandardScaler() # MinMaxScaler or StandardScaler\n#X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])\n#X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])\n\nprint(f\"X_train shape :{X_train.shape} , y_train shape :{Y_train.shape}\")\nprint(f\"X_test shape :{X_test.shape}\")\n\n# Delete the train and test dataframes to free up memory\n# del df_train, df_test\n\nX_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-15T20:13:17.021581Z","iopub.execute_input":"2023-06-15T20:13:17.022058Z","iopub.status.idle":"2023-06-15T20:13:17.102369Z","shell.execute_reply.started":"2023-06-15T20:13:17.022035Z","shell.execute_reply":"2023-06-15T20:13:17.101079Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"X_train shape :(617, 56) , y_train shape :(617,)\nX_test shape :(5, 56)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"         AB          AF          AH         AM        AR        AX        AY  \\\n0  0.209377  3109.03329   85.200147  22.394407  8.138688  0.699861  0.025578   \n1  0.145282   978.76416   85.200147  36.968889  8.138688  3.632190  0.025578   \n2  0.470030  2635.10654   85.200147  32.360553  8.138688  6.732840  0.025578   \n3  0.252107  3819.65177  120.201618  77.112203  8.138688  3.685344  0.025578   \n4  0.380297  3733.04844   85.200147  14.103738  8.138688  3.942255  0.054810   \n\n          AZ          BC         BD   ...         FI        FL        FR  \\\n0   9.812214    5.555634  4126.58731  ...   3.583450  7.298162   1.73855   \n1  13.517790    1.229900  5496.92824  ...  10.358927  0.173229   0.49706   \n2  12.824570    1.229900  5135.78024  ...  11.626917  7.709560   0.97556   \n3  11.053708    1.229900  4169.67738  ...  14.852022  6.122162   0.49706   \n4   3.396778  102.151980  5728.73412  ...  13.666727  8.153058  48.50134   \n\n         FS         GB          GE            GF         GH         GI  \\\n0  0.094822  11.339138   72.611063   2003.810319  22.136229  69.834944   \n1  0.568932   9.292698   72.611063  27981.562750  29.135430  32.131996   \n2  1.198821  37.077772   88.609437  13676.957810  28.022851  35.192676   \n3  0.284466  18.529584   82.416803   2094.262452  39.948656  90.493248   \n4  0.121914  16.408728  146.109943   8524.370502  45.381316  36.262628   \n\n          GL  \n0   0.120343  \n1  21.978000  \n2   0.196941  \n3   0.155829  \n4   0.096614  \n\n[5 rows x 56 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AB</th>\n      <th>AF</th>\n      <th>AH</th>\n      <th>AM</th>\n      <th>AR</th>\n      <th>AX</th>\n      <th>AY</th>\n      <th>AZ</th>\n      <th>BC</th>\n      <th>BD</th>\n      <th>...</th>\n      <th>FI</th>\n      <th>FL</th>\n      <th>FR</th>\n      <th>FS</th>\n      <th>GB</th>\n      <th>GE</th>\n      <th>GF</th>\n      <th>GH</th>\n      <th>GI</th>\n      <th>GL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.209377</td>\n      <td>3109.03329</td>\n      <td>85.200147</td>\n      <td>22.394407</td>\n      <td>8.138688</td>\n      <td>0.699861</td>\n      <td>0.025578</td>\n      <td>9.812214</td>\n      <td>5.555634</td>\n      <td>4126.58731</td>\n      <td>...</td>\n      <td>3.583450</td>\n      <td>7.298162</td>\n      <td>1.73855</td>\n      <td>0.094822</td>\n      <td>11.339138</td>\n      <td>72.611063</td>\n      <td>2003.810319</td>\n      <td>22.136229</td>\n      <td>69.834944</td>\n      <td>0.120343</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.145282</td>\n      <td>978.76416</td>\n      <td>85.200147</td>\n      <td>36.968889</td>\n      <td>8.138688</td>\n      <td>3.632190</td>\n      <td>0.025578</td>\n      <td>13.517790</td>\n      <td>1.229900</td>\n      <td>5496.92824</td>\n      <td>...</td>\n      <td>10.358927</td>\n      <td>0.173229</td>\n      <td>0.49706</td>\n      <td>0.568932</td>\n      <td>9.292698</td>\n      <td>72.611063</td>\n      <td>27981.562750</td>\n      <td>29.135430</td>\n      <td>32.131996</td>\n      <td>21.978000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.470030</td>\n      <td>2635.10654</td>\n      <td>85.200147</td>\n      <td>32.360553</td>\n      <td>8.138688</td>\n      <td>6.732840</td>\n      <td>0.025578</td>\n      <td>12.824570</td>\n      <td>1.229900</td>\n      <td>5135.78024</td>\n      <td>...</td>\n      <td>11.626917</td>\n      <td>7.709560</td>\n      <td>0.97556</td>\n      <td>1.198821</td>\n      <td>37.077772</td>\n      <td>88.609437</td>\n      <td>13676.957810</td>\n      <td>28.022851</td>\n      <td>35.192676</td>\n      <td>0.196941</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.252107</td>\n      <td>3819.65177</td>\n      <td>120.201618</td>\n      <td>77.112203</td>\n      <td>8.138688</td>\n      <td>3.685344</td>\n      <td>0.025578</td>\n      <td>11.053708</td>\n      <td>1.229900</td>\n      <td>4169.67738</td>\n      <td>...</td>\n      <td>14.852022</td>\n      <td>6.122162</td>\n      <td>0.49706</td>\n      <td>0.284466</td>\n      <td>18.529584</td>\n      <td>82.416803</td>\n      <td>2094.262452</td>\n      <td>39.948656</td>\n      <td>90.493248</td>\n      <td>0.155829</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.380297</td>\n      <td>3733.04844</td>\n      <td>85.200147</td>\n      <td>14.103738</td>\n      <td>8.138688</td>\n      <td>3.942255</td>\n      <td>0.054810</td>\n      <td>3.396778</td>\n      <td>102.151980</td>\n      <td>5728.73412</td>\n      <td>...</td>\n      <td>13.666727</td>\n      <td>8.153058</td>\n      <td>48.50134</td>\n      <td>0.121914</td>\n      <td>16.408728</td>\n      <td>146.109943</td>\n      <td>8524.370502</td>\n      <td>45.381316</td>\n      <td>36.262628</td>\n      <td>0.096614</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 56 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Split and Hyperparameter config","metadata":{}},{"cell_type":"code","source":"class Splitter:\n    def __init__(self, n_splits = 5):\n        self.n_splits = n_splits\n        \n    def split_data(self, X, y, random_state_list):\n        for random_state in random_state_list:\n            # shuffle = True ???\n            kfold = StratifiedKFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n            for tr_index, val_index in kfold.split(X, y):\n                yield tr_index, val_index\n\nclass Classifier:\n    def __init__(self, n_estimators = 100, device = 'cpu', random_state = 0):\n        self.n_estimators = n_estimators\n        self.device = device\n        self.random_state = random_state\n        self.models = self._define_models()\n        self.models_name = list(self.models.keys())\n        self.len_models = len(self.models)\n        \n    \n    def _define_models(self):\n        logistic_param = {\n            'random_state': self.random_state,\n            'penalty': 'l2',\n            #'l1_ratio': 0\n        }\n        \n        xgb1_params = {\n            'n_estimators': self.n_estimators,\n            'learning_rate': 0.413327571405248,\n            'booster': 'gbtree',\n            'lambda': 0.0000263894617720096,\n            'alpha': 0.000463768723479341,\n            'subsample': 0.237467672874133,\n            'colsample_bytree': 0.618829300507829,\n            'max_depth': 5,\n            'min_child_weight': 9,\n            'eta': 2.09477807126539E-06,\n            'gamma': 0.000847289463422307,\n            'grow_policy': 'depthwise',\n            'n_jobs': -1,\n            'objective': 'binary:logistic',\n            'eval_metric': 'logloss',\n            'verbosity': 0,\n            'random_state': self.random_state,\n        }\n        if self.device == 'gpu':\n            xgb_params['tree_method'] = 'gpu_hist'\n            xgb_params['predictor'] = 'gpu_predictor'\n        \n        lgb1_params = {\n            'n_estimators': self.n_estimators,\n            'objective': 'binary',\n            'boosting_type': 'gbdt',\n            'learning_rate': 0.005,\n            'num_leaves': 5,\n            'colsample_bytree': 0.50,\n            'subsample': 0.80,\n            'reg_alpha': 2, \n            'reg_lambda': 4,\n            'n_jobs': -1,\n            'is_unbalance':True,\n            'device': self.device,\n            'random_state': self.random_state\n        }\n        lgb2_params = {\n            'n_estimators': self.n_estimators,\n            'learning_rate': 0.190197487721534,\n            'reg_alpha': 0.00749112221417973,\n            'reg_lambda': 0.000548118227209224,\n            'num_leaves': 17,\n            'colsample_bytree': 0.547257860506146,\n            'subsample': 0.592628085686409,\n            'subsample_freq': 2,\n            'min_child_samples': 64,\n            'objective': 'binary',\n            #'metric': 'binary_error',\n            'boosting_type': 'gbdt',\n            'is_unbalance':True,\n            'device': self.device,\n            'random_state': self.random_state\n        } \n        lgb3_params = {\n            'n_estimators': self.n_estimators,\n            'learning_rate': 0.181326407627473,\n            'reg_alpha': 0.000030864084239014,\n            'reg_lambda': 0.0000395714763869486,\n            'num_leaves': 122,\n            'colsample_bytree': 0.75076596295323,\n            'subsample': 0.6303245788342,\n            'subsample_freq': 3,\n            'min_child_samples': 72,\n            'objective': 'binary',\n            #'metric': 'binary_error',\n            'boosting_type': 'gbdt',\n            'is_unbalance':True,\n            'device': self.device,\n            'random_state': self.random_state\n        } \n        cat1_params = {\n            'iterations': self.n_estimators,\n            'colsample_bylevel': 0.0513276895988184,\n            'depth': 2,\n            'learning_rate': 0.0256579773375401,\n            'l2_leaf_reg': 8.22319805476255,\n            'random_strength': 0.11327724457066,\n            'od_type': \"Iter\", \n            'od_wait': 72,\n            'bootstrap_type': \"Bayesian\",\n            'grow_policy': 'SymmetricTree',\n            'bagging_temperature': 9.58737431845122,\n            #'eval_metric': 'Logloss',\n            #'loss_function': 'Logloss',\n            'auto_class_weights': 'Balanced',\n            'task_type': self.device.upper(),\n            'random_state': self.random_state\n        }\n        \n        study = optuna.create_study(direction='minimize')\n        study.optimize(self.lgb_optuna, n_trials=20)\n        lgb4_params = study.best_params\n        lgb4_params['objective'] = 'binary'\n        #lgb4_params['learning_rate'] = 0.1\n        lgb4_params['is_unbalance'] = True\n        \n        models = {\n            #'logistic': LogisticRegression(**logistic_param),\n            #'xgb1': xgb.XGBClassifier(**xgb1_params),\n            'lgb1': lgb.LGBMClassifier(**lgb1_params),\n            #'lgb2': lgb.LGBMClassifier(**lgb2_params),\n            #'lgb3': lgb.LGBMClassifier(**lgb3_params),\n            'lgb4': lgb.LGBMClassifier(**lgb4_params),\n            #'cat1': CatBoostClassifier(**cat1_params),\n        }\n        \n        return models\n    \n    def lgb_optuna(self, trial):\n        params = {\n            'objective': 'binary',\n            #'metric': 'gbdt',\n            'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.5),\n            'num_leaves': trial.suggest_int('num_leaves', 3, 100),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n            'max_depth':trial.suggest_int('max_depth', 1, 20),\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n            'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 10.0),\n            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0),\n            'is_unbalance':True\n        }\n\n        score_list = []\n\n        for fold, (train_idx, val_idx) in enumerate(StratifiedKFold(n_splits=5, random_state=42, shuffle=True).split(X_train, Y_train)):\n            x_tr, x_va = X_train.loc[train_idx], X_train.loc[val_idx]\n            y_tr, y_va = Y_train.loc[train_idx], Y_train.loc[val_idx]\n            train_w0, train_w1 = calc_log_loss_weight(y_tr)\n            valid_w0, valid_w1 = calc_log_loss_weight(y_va)\n\n            model = lgb.LGBMClassifier(**params)\n            model.fit(x_tr, y_tr, sample_weight=y_tr.map({0: train_w0, 1: train_w1}), \n                      eval_set=[(x_va,y_va)], eval_sample_weight=[y_va.map({0: valid_w0, 1: valid_w1})],\n                      verbose=0, early_stopping_rounds=500)\n\n            y_va_pred = model.predict_proba(x_va)[:, 1].reshape(-1)\n            score = balanced_log_loss(y_va, y_va_pred)\n            score_list.append(score)\n\n        return sum(score_list) / len(score_list)","metadata":{"execution":{"iopub.status.busy":"2023-06-13T20:55:32.973312Z","iopub.execute_input":"2023-06-13T20:55:32.973736Z","iopub.status.idle":"2023-06-13T20:55:32.999460Z","shell.execute_reply.started":"2023-06-13T20:55:32.973706Z","shell.execute_reply":"2023-06-13T20:55:32.998100Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"def calc_log_loss_weight(y_true):\n    nc = np.bincount(y_true)\n    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n    # w0, w1 = 1/nc[0], 1/nc[1]\n    return w0, w1\n\ndef balanced_log_loss(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n    nc = np.bincount(y_true)\n    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n    balanced_log_loss_score = (-w0/nc[0]*(np.sum(np.where(y_true==0,1,0) * np.log(1-y_pred))) - w1/nc[1]*(np.sum(np.where(y_true!=0,1,0) * np.log(y_pred)))) / (w0+w1)\n    return balanced_log_loss_score\n\ndef balanced_log_loss(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n    nc = np.bincount(y_true)\n    w0, w1 = 1/nc[0], 1/nc[1]\n    balanced_log_loss_score = (-w0*(np.sum(np.where(y_true==0,1,0) * np.log(1-y_pred))) - w1*(np.sum(np.where(y_true!=0,1,0) * np.log(y_pred)))) / 2\n    return balanced_log_loss_score","metadata":{"execution":{"iopub.status.busy":"2023-06-13T20:55:33.001831Z","iopub.execute_input":"2023-06-13T20:55:33.002161Z","iopub.status.idle":"2023-06-13T20:55:33.015531Z","shell.execute_reply.started":"2023-06-13T20:55:33.002133Z","shell.execute_reply":"2023-06-13T20:55:33.014207Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"%%time\n\nn_splits = 5\nn_reapts = 1\nrandom_state = 42\nn_estimators = 99999\nearly_stopping_rounds = 1000\nverbose = False\ndevice = 'cpu'\n\n# Fix seed\nrandom.seed(random_state)\nrandom_state_list = random.sample(range(9999), n_reapts)\n\n# Initialize an array for storing test predictions\nclassifier = Classifier(n_estimators, device, random_state)\noof_stack = pd.DataFrame(np.zeros(X_train.shape[0]), columns=['oof_stack'])\ntest_stack = pd.DataFrame(np.zeros(X_test.shape[0]), columns=['test_stack'])\noof_pred = pd.DataFrame(np.zeros((X_train.shape[0], classifier.len_models)), columns=classifier.models_name)\ntest_pred = pd.DataFrame(np.zeros((X_test.shape[0], classifier.len_models)), columns=classifier.models_name)\ntrained_models = {'xgb':[], 'cat':[]}\nscore_dict = dict(zip(classifier.models_name, [[] for _ in range(classifier.len_models)]))\n\nsplitter = Splitter(n_splits=n_splits)\nfor i, (tr_idx, val_idx) in enumerate(splitter.split_data(X_train, Y_train, random_state_list=random_state_list)):\n    x_train, x_val = X_train.loc[tr_idx], X_train.loc[val_idx]\n    y_train, y_val = Y_train.loc[tr_idx], Y_train.loc[val_idx]\n    n = i % n_splits\n    m = i // n_splits\n            \n    # Get a set of classifier models\n    classifier = Classifier(n_estimators, device, random_state_list[m])\n    models = classifier.models\n    \n    # Initialize lists to store oof and test predictions for each base model\n    \n    # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n    print(f'************** Start Training Fold-{n} ******************')\n    for name, model in models.items():\n        if ('xgb' in name) or ('lgb' in name) or ('cat' in name):\n            train_w0, train_w1 = calc_log_loss_weight(y_train)\n            valid_w0, valid_w1 = calc_log_loss_weight(y_val)\n            if 'xgb' in name:\n                model.fit(\n                    x_train, y_train, sample_weight=y_train.map({0: train_w0, 1: train_w1}), \n                    eval_set=[(x_val, y_val)], sample_weight_eval_set=[y_val.map({0: valid_w0, 1: valid_w1})],\n                    early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n            elif 'lgb' in name:\n                model.fit(\n                    x_train, y_train, sample_weight=y_train.map({0: train_w0, 1: train_w1}), \n                    eval_set=[(x_val, y_val)], eval_sample_weight=[y_val.map({0: valid_w0, 1: valid_w1})],\n                    early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n            elif 'cat' in name:\n                model.fit(\n                    Pool(x_train, y_train, weight=y_train.map({0: train_w0, 1: train_w1})), \n                    eval_set=Pool(x_val, y_val, weight=y_val.map({0: valid_w0, 1: valid_w1})), \n                    early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n        else:\n            model.fit(x_train, y_train)\n            \n        if name in trained_models.keys():\n            trained_models[f'{name}'].append(deepcopy(model))\n        \n        y_train_pred = model.predict_proba(x_train)[:, 1].reshape(-1)\n        y_val_pred = model.predict_proba(x_val)[:, 1].reshape(-1)\n        y_test_pred = model.predict_proba(X_test)[:, 1].reshape(-1)\n        \n        train_score = balanced_log_loss(y_train, y_train_pred)\n        val_score = balanced_log_loss(y_val, y_val_pred)\n        #score_dict[name].append(score)\n        \n        print(f'{name} [REPEAT-{m} FOLD-{n} SEED-{random_state_list[m]}] BalancedLogLoss Validation score: {val_score:.5f}, Training score: {train_score:.5f}')\n        \n        #oof_preds.append(y_val_pred)\n        oof_pred[name].loc[x_val.index] = y_val_pred\n        #test_preds.append(test_pred)\n        test_pred[name] += y_test_pred / n_splits\n    \n    print('\\n')\n\nfor name in classifier.models_name:\n    cv_score = balanced_log_loss(Y_train, oof_pred[name])\n    print(f'{name} SEED-{random_state_list[m]}] BalancedLogLoss Total CV score: {cv_score:.5f}')\n#oof_each_predss = np.mean(np.array(oof_each_predss), axis=0)\n#test_each_predss = np.mean(np.array(test_each_predss), axis=0)\n#oof_each_predss = np.concatenate([oof_each_predss, np.mean(oof_predss, axis=1).reshape(-1, 1)], axis=1)\n#test_each_predss = np.concatenate([test_each_predss, test_predss.reshape(-1, 1)], axis=1)\n\n# Comducted a simple linear regression to stack all the models\nprint('\\n')\nprint(f'************** STACKING ******************')\n#lr = LinearRegression(positive=True, fit_intercept=False).fit(oof_pred, Y_train)\noof_stack['oof_stack'] = oof_pred['lgb4']#lr.predict(oof_pred) / sum(lr.coef_)\ntest_stack['test_stack'] = test_pred['lgb4']#lr.predict(test_pred) / sum(lr.coef_) \nstack_score = balanced_log_loss(Y_train, oof_stack['oof_stack'])\nprint(f'Stacking SEED-{random_state_list[m]}] BalancedLogLoss Total CV score: {stack_score:.5f}')","metadata":{"execution":{"iopub.status.busy":"2023-06-13T20:55:33.019356Z","iopub.execute_input":"2023-06-13T20:55:33.019718Z","iopub.status.idle":"2023-06-13T20:57:41.046408Z","shell.execute_reply.started":"2023-06-13T20:55:33.019688Z","shell.execute_reply":"2023-06-13T20:57:41.045244Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"************** Start Training Fold-0 ******************\nlgb1 [REPEAT-0 FOLD-0 SEED-1824] BalancedLogLoss Validation score: 0.19569, Training score: 0.03745\nlgb4 [REPEAT-0 FOLD-0 SEED-1824] BalancedLogLoss Validation score: 0.16201, Training score: 0.02509\n\n\n************** Start Training Fold-1 ******************\nlgb1 [REPEAT-0 FOLD-1 SEED-1824] BalancedLogLoss Validation score: 0.18362, Training score: 0.02603\nlgb4 [REPEAT-0 FOLD-1 SEED-1824] BalancedLogLoss Validation score: 0.22221, Training score: 0.05737\n\n\n************** Start Training Fold-2 ******************\nlgb1 [REPEAT-0 FOLD-2 SEED-1824] BalancedLogLoss Validation score: 0.22252, Training score: 0.03153\nlgb4 [REPEAT-0 FOLD-2 SEED-1824] BalancedLogLoss Validation score: 0.22145, Training score: 0.02913\n\n\n************** Start Training Fold-3 ******************\nlgb1 [REPEAT-0 FOLD-3 SEED-1824] BalancedLogLoss Validation score: 0.27464, Training score: 0.04153\nlgb4 [REPEAT-0 FOLD-3 SEED-1824] BalancedLogLoss Validation score: 0.26785, Training score: 0.10027\n\n\n************** Start Training Fold-4 ******************\nlgb1 [REPEAT-0 FOLD-4 SEED-1824] BalancedLogLoss Validation score: 0.31165, Training score: 0.08545\nlgb4 [REPEAT-0 FOLD-4 SEED-1824] BalancedLogLoss Validation score: 0.30309, Training score: 0.10210\n\n\nlgb1 SEED-1824] BalancedLogLoss Total CV score: 0.23681\nlgb4 SEED-1824] BalancedLogLoss Total CV score: 0.23479\n\n\n************** STACKING ******************\nStacking SEED-1824] BalancedLogLoss Total CV score: 0.23479\nCPU times: user 2min 19s, sys: 1min 2s, total: 3min 21s\nWall time: 2min 8s\n","output_type":"stream"}]},{"cell_type":"code","source":"sub = pd.read_csv(os.path.join(filepath, 'sample_submission.csv'))\n\nsub['class_1'] = test_stack['test_stack']\nsub['class_0'] = 1 - test_stack['test_stack']\nsub.to_csv('submission.csv', index=False)\nsub\n\n#0.23697\n#0.23414\n#0.24283","metadata":{"execution":{"iopub.status.busy":"2023-06-13T20:57:41.048104Z","iopub.execute_input":"2023-06-13T20:57:41.048415Z","iopub.status.idle":"2023-06-13T20:57:41.066832Z","shell.execute_reply.started":"2023-06-13T20:57:41.048389Z","shell.execute_reply":"2023-06-13T20:57:41.065639Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"             Id   class_0   class_1\n0  00eed32682bb  0.722936  0.277064\n1  010ebe33f668  0.722936  0.277064\n2  02fa521e1838  0.722936  0.277064\n3  040e15f562a2  0.722936  0.277064\n4  046e85c7cc7f  0.722936  0.277064","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>class_0</th>\n      <th>class_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00eed32682bb</td>\n      <td>0.722936</td>\n      <td>0.277064</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>010ebe33f668</td>\n      <td>0.722936</td>\n      <td>0.277064</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>02fa521e1838</td>\n      <td>0.722936</td>\n      <td>0.277064</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>040e15f562a2</td>\n      <td>0.722936</td>\n      <td>0.277064</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>046e85c7cc7f</td>\n      <td>0.722936</td>\n      <td>0.277064</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}